# Research meeting: 10/05

## Status
* That CBPV with pointers reflects the distinction between thunks (suspended computations) and values is *immensely* convenient for us, but it does pose an interesting question for Haskell’s call-by-need evaluation strategy; should we reflect the transformation from thunk to value in the types (somehow) or consider “forcing" a pointed-to thunk (as an operation distinct from the basic force operation in CBPV) as an implicit update in the heap?
	* The latter *might* actually be the intended semantics of forcing a computation pointer in HA-CBPV; I don’t feel confident without a dynamic semantics.
		* Are the dynamic semantics implied by the rulesets given? Perhaps I just don’t understand the typical approach used in a CBPV formalism.
* The CompCert memory model is way more sophisticated than I expected, but I think that we can adapt it rather conservatively and abstract way the worst of its complexity: Rather than model the heap as memory blocks of "bytes” (really a closed dynamic type of primitive memory cells plus bookkeeping metadata), we could model each memory block as a finite map from “fields” (*i.e.*, a set of allowed offsets fixed at allocation time) to such primitive memory cells.
	* I have been referring to the [original publication](refs/compcert.memory.v1.pdf).
	* *Why would we bother?* The CompCert memory model is almost delightfully simple, *except* everything regarding byte-level offsets.
		* For example, storing a 32-bit integer at some offset means inserting the integer at that offset’s *byte* memory cell, then setting three subsequent byte memory cells to indicate that they are conceptually used by that first byte’s memory cell. Complex bounds checking abounds from this fact.
	* *What would we change?* Offsets into a memory block (*e.g.*, `offsetof(struct_type, field_name)`) would get treated as opaque constants; the machine model will have no insight into how those offsets correspond to integers. Pointer arithmetic is then only well defined if the integer offset equals some such opaque constant, but this equality check could be a parametric detail of our CBPV model, so one could in principle prove theorems requiring a more concrete instantiation of pointer offset arithmetic.
		* Describing `malloc` will be awkward at best with this memory model, but `new` will be trivial.
			* We *could* model `malloc`, if we also treat `sizeof(struct_type)` as an opaque constant, thus fixing the type of the memory block at allocation time.
* Lucy appears to be off Slack during her internship, so I wonder whether Ott is a decent way to formalize our CBPV machine model and its relation with Core Haskell and κ++?
	* This could potentially satisfy my urge for mechanization without getting us bogged down in minutiae.
	* Ott would simplify – though not automate – deriving executable models of the various calculi in play.
	* In the meantime, I’ve just been sketching math on pen and paper; if Ott is overkill, I can typeset into LaTeX (already started but not yet complete).
* Theo suggested two tasks for me to focus our attention and priorities:
    1. Summarize the proof obligations for the [realizability semantics of interoperability](refs/interop.sound.pdf)
        * Refer also to [Daniel's dissertation](refs/interop.realiz.pdf)
    2. Summarize the [call-by-push-value model of the LLVM machine model](refs/cbpv.ssa.pdf)
        * Refer also to [the later ArXiv paper](refs/cbpv.cfg.pdf)
## Minutes

## Follow up

